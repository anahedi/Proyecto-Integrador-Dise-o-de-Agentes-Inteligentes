# -*- coding: utf-8 -*-
"""DQL_Walle.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vq3DV7rfbiFRV24LUaGVgJUf0mYdubIR
"""

#Instalamos todo lo necesario
#Hay que correr esta celda solo la primera vez que
#comenzamos a trabajar

!pip install --upgrade mesa[all]
!pip install seaborn

#Importamos todas las librerias
#Hay que correr esta celda cada vez que agreguemos
#una nueva libreria

import json
import random
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import matplotlib.pyplot as plt


from mesa import Agent, Model
from mesa.space import MultiGrid

from collections import deque

# Cargar datos desde el archivo JSON
with open("world_5x5.json") as file:
    data = json.load(file)


# Extraer parámetros del JSON
GRID_WIDTH = data["width"]
GRID_HEIGHT = data["height"]
START_X, START_Y = data["start"]
GOAL_X, GOAL_Y = data["goal"]

# Convertir los obstáculos en un diccionario con penalización fija
OBSTACLE_PENALTY = -5  # Penalización por chocar con un obstáculo
obstacles = {(x, y): OBSTACLE_PENALTY for x, y in data["obstacles"]}
print("Obstáculos cargados con penalización:", obstacles)

# Definimos la red neuronal para DQL
class DQN(nn.Module):
    def __init__(self, input_dim, output_dim):
        super(DQN, self).__init__()
        self.fc1 = nn.Linear(input_dim, 64)
        self.fc2 = nn.Linear(64, 64)
        self.fc3 = nn.Linear(64, output_dim)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        return self.fc3(x)

#Implementamos los agentes

class Walle_DQL(Agent):
    """Agent that moves randomly until it reaches the goal or fails after max steps."""

    def __init__(self, model, x, y, goalx, goaly):
        super().__init__(model)
        self.x = x
        self.y = y
        self.spawn_x = x  # Guardamos la posicion inicial para generar el json
        self.spawn_y = y
        self.goalx = goalx  #Guardamos la meta
        self.goaly = goaly
        self.steps_taken = 0  #Como nos estamos moviendo de manera aleatoria, necesitamos detenernos en algun momento
        self.path = []  # Guardamos todos los pasos que tomamos para hacer el path en UNITY
        self.meta_alcanzada = False  # Añadimos la bandera
        self.obstacles = obstacles
        self.cells_explored = set()  # 🔹 Set para contar celdas únicas exploradas

        self.state_size = 2  # Posición (x, y)
        self.action_size = 4  # {arriba, abajo, izquierda, derecha}
        self.epsilon = 1.0  # Exploración inicial
        self.epsilon_min = 0.01
        self.epsilon_decay = 0.995
        self.gamma = 0.8
        self.learning_rate = 0.0001
        self.memory = deque(maxlen=2000)


        # Redes DQN y Target Network
        self.model_dqn = DQN(self.state_size, self.action_size)
        self.target_network = DQN(self.state_size, self.action_size)  # Red objetivo
        self.target_network.load_state_dict(self.model_dqn.state_dict())  # Inicializamos con los mismos pesos
        self.target_network.eval()  # No entrenamos la red objetivo directamente

        self.optimizer = optim.Adam(self.model_dqn.parameters(), lr=self.learning_rate)
        self.update_target_counter = 0  # Contador para actualizar la red objetivo


          # 🔹 Listas para graficar el progreso
        self.episode_rewards = []
        self.losses = []
        self.epsilons = []


    def act(self, state):
        if np.random.rand() <= self.epsilon:
            return random.choice(range(self.action_size))  # Exploración
        state_tensor = torch.FloatTensor(state).unsqueeze(0)
        q_values = self.model_dqn(state_tensor)
        return torch.argmax(q_values).item()  # Acción con mayor Q

    def step(self):
        total_reward = 0


        #Aqui vamos a poner las cosas que haremos cada paso, como imprimir el camino y revisar si ya nos
        #Pasamos de pasos para detenernos


        #Justo aqui revisamos si ya nos pasamos de pasos


        #moves = []

        while not self.meta_alcanzada:  # Sigue explorando hasta alcanzar la meta
          state = np.array([self.x, self.y])
          action = self.act(state)


          # Mapeamos acción a movimiento
          moves = [(0, -1), (0, 1), (-1, 0), (1, 0)]
          new_x, new_y = self.x + moves[action][0], self.y + moves[action][1]

          """
          #Percibimos el tamaño del grid
          grid_width, grid_height = self.model.grid.width, self.model.grid.height

          # Revisamos si me puedo mover para esa direccion
          if self.x < grid_width - 1:
              moves.append((self.x + 1, self.y))
          if self.x > 0:
              moves.append((self.x - 1, self.y))
          if self.y < grid_height - 1:
              moves.append((self.x, self.y + 1))
          if self.y > 0:
              moves.append((self.x, self.y - 1))
  """


          # Verificamos que el movimiento es válido
          if 0 <= new_x < self.model.grid.width and 0 <= new_y < self.model.grid.height and ( (new_x, new_y) not in self.obstacles):
              self.model.grid.move_agent(self, (new_x, new_y))
              #print(f"new_x: {new_x}, new_y: {new_y}, self.goalx: {self.goalx}, self.goaly: {self.goaly}") #aqui
              if (new_x, new_y) == (self.goalx, self.goaly):
                  reward = 10  # Recompensa por llegar a la meta
                  self.meta_alcanzada = True
              elif (new_x, new_y) in obstacles:
                  reward = obstacles[(new_x, new_y)]  # Penalización del JSON
              else:
                  prev_distance = abs(self.x - self.goalx) + abs(self.y - self.goaly)
                  new_distance = abs(new_x - self.goalx) + abs(new_y - self.goaly)

                  reward = -1  # Penalización base por moverse
                  if new_distance < prev_distance:
                      reward += 1  # Recompensa adicional por acercarse a la meta

                     # 🔹 Agregar celda explorada al set
                  self.cells_explored.add((self.x, self.y))
                  done = (new_x, new_y) == (self.goalx, self.goaly)

                  next_state = np.array([new_x, new_y])
                  self.memory.append((state, action, reward, next_state, done))

                #Actualizamos la posición del agente
              self.x, self.y = new_x, new_y
                #Sumamos un paso
              self.steps_taken += 1
              #Guardo el paso que di
              self.path.append({"x": self.x, "y": self.y})
                #Imprimo a donde me movi
              print(f"Walle_reactivo moved to ({self.x}, {self.y})")

              total_reward += reward

              if not self.meta_alcanzada: #aqui
                  self.replay() #entrena la red con experiencias pasadas

              if self.epsilon > self.epsilon_min:
                  self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)

         # print(f"Exploración finalizada. Celdas exploradas: {len(self.cells_explored)}")
          self.episode_rewards.append(total_reward)

    def replay(self, batch_size=16):
        if len(self.memory) < batch_size:
            return

        total_loss = 0.0
        minibatch = random.sample(self.memory, batch_size)

        for state, action, reward, next_state, done in minibatch:
            state_tensor = torch.FloatTensor(state).unsqueeze(0)
            next_state_tensor = torch.FloatTensor(next_state).unsqueeze(0)

            target = reward
            target = reward + self.gamma * torch.max(self.target_network(next_state_tensor)) if not done else reward


            # Imprimir target calculado
            #print(f"Target: {target.item()}")

            # Convertimos target a tensor flotante
            target = torch.tensor(target, dtype=torch.float)

            predicted_q_values = self.model_dqn(state_tensor)
            predicted_q_value = predicted_q_values[0][action]

            # Imprimir valores Q predichos
           # print(f"Predicted Q-values: {predicted_q_values}")
           # print(f"Predicted Q-value for action {action}: {predicted_q_value.item()}")

            # Calculamos la pérdida
            loss = F.mse_loss(predicted_q_value, target)  # Loss sobre el tensor target

            # Imprimir la pérdida
            #8317print(f"Loss: {loss.item()}")  # Imprime la pérdida

            self.optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(self.model_dqn.parameters(), max_norm=1.0)
            self.optimizer.step()

            total_loss += loss.item()
        # Actualizamos la red objetivo cada ciertos pasos
        self.update_target_counter += 1
        tau = 0.01  # Controla qué tanto se actualiza la red objetivo
        for target_param, model_param in zip(self.target_network.parameters(), self.model_dqn.parameters()):
          target_param.data.copy_(tau * model_param.data + (1 - tau) * target_param.data)

        # Imprimir la pérdida promedio
        print(f"Average Loss: {total_loss / batch_size}")

        return total_loss / batch_size

    def track_episode_reward(self, episode_reward):
        """Llamar después de cada episodio para guardar la recompensa obtenida."""
        self.episode_rewards.append(episode_reward)

    def plot_results(self):
            """Genera los gráficos de recompensa y pérdida promedio por episodio."""
            episodes = range(len(self.episode_rewards))

            # Graficar recompensa total por episodio
            plt.figure(figsize=(12, 6))
            plt.subplot(1, 2, 1)
            plt.plot(episodes, self.episode_rewards, label='Recompensa Total')
            plt.xlabel('Episodios')
            plt.ylabel('Recompensa Total')
            plt.title('Recompensa Total por Episodio')
            plt.grid(True)

            # Graficar pérdida promedio por episodio
            plt.subplot(1, 2, 2)
            plt.plot(episodes, self.losses, label='Pérdida Promedio', color='r')
            plt.xlabel('Episodios')
            plt.ylabel('Pérdida Promedio')
            plt.title('Pérdida Promedio por Episodio')
            plt.grid(True)

            # Mostrar el gráfico
            plt.tight_layout()
            plt.show()

#Implementamos el ambiente, al cual tambien agregamos los agentes
#El ambiente es discreto por lo que usamos un grid.

class GridModel(Model):
    """A grid model containing reactive agents."""

    def __init__(self, width, height, goalx, goaly):
        super().__init__()
        self.grid = MultiGrid(width, height, torus=False)

        # Creamos el agente reactivo y le digo que va a empezar en 0,0
        #Con Máximo 25 pasos para llegar a la meta
        self.agent1 = Walle_reactivo(self, 0, 0, goalx, goaly)
        #Lo pongo en el grid en el inicio
        self.grid.place_agent(self.agent1, (0, 0))

    def step(self):
        """Advance the model by one step."""
        #Caminamos un paso del agente, si tuviera mas agentes, aqui camino todos
        #esos pasos

        self.agent1.step()

    def save_log(self, filename="walle_log.json"):
        """Save movement history to a JSON file in the required format."""
        #Genero el JSON para UNITY en el formato que ya estamos usando en
        #El test bed

        log_data = {
            "robots": [
                {
                    "spawnPosition": {
                        "x": self.agent1.spawn_x,
                        "y": self.agent1.spawn_y
                    },
                    "path": self.agent1.path
                }
            ]
        }
        with open(filename, "w") as file:
            json.dump(log_data, file, indent=4)
        print(f"Simulation log saved to {filename}")

# Creamos la instancia del modelo
model = GridModel(GRID_WIDTH, GRID_HEIGHT, GOAL_X, GOAL_Y)

# Número de episodios de entrenamiento
num_episodes = 10
update_target_every = 20  # Cada cuántos episodios actualizar la red objetivo

# Inicializar listas de seguimiento
model.agent1.episode_rewards = []
model.agent1.losses = []

for episode in range(num_episodes):
    print(f"\n🔹 Episodio {episode + 1}/{num_episodes}")

    # Reiniciar el estado del agente
    model.agent1.steps_taken = 0
    model.agent1.x, model.agent1.y = 0, 0  # Volver al inicio
    model.agent1.path = []
    model.agent1.meta_alcanzada = False
    episode_reward = 0

    # Usar while sin límite de pasos, solo terminamos cuando el agente llega a la meta
    while not model.agent1.meta_alcanzada:
        if model.agent1.meta_alcanzada:
            break  # Terminar el episodio si la meta fue alcanzada

        model.step()

        # Revisar si llegó a la meta
        if (model.agent1.x, model.agent1.y) == (model.agent1.goalx, model.agent1.goaly):
            model.agent1.meta_alcanzada = True
            episode_reward += 10  # Recompensa por llegar a la meta
        #episode_reward += model.agent1.reward
    # Guardar recompensa del episodio
    #model.agent1.episode_rewards.append(episode_reward)

    # Actualizar la red objetivo cada `update_target_every` episodios
    if episode % update_target_every == 0:
        model.agent1.target_network.load_state_dict(model.agent1.model_dqn.state_dict())

    # Imprimir información periódicamente
    #if episode % 100 == 0:
       # print(f"📊 Episodio: {episode}, Recompensa: {episode_reward}, Epsilon: {model.agent1.epsilon}")

    # Guardar logs
    model.save_log()

# Graficar resultados después del entrenamiento
model.agent1.plot_results()

print(f"Celdas únicas exploradas hasta ahora: {len(model.agent1.cells_explored)}")

print(f"🔹 Recorrido completo del agente: {model.agent1.path}")
print(f"🔹 Número de pasos dados: {model.agent1.steps_taken}")

#Corremos la solucion
# -------------------------
# RUNNING THE SIMULATION
# -------------------------
"""
# Definimos la maya y la meta del agente
GRID_WIDTH = 10
GRID_HEIGHT = 10
GOAL_X = 9
GOAL_Y = 9"""

# Creamos una instancia del modelo para poder ejecutarlo
model = GridModel(GRID_WIDTH, GRID_HEIGHT, GOAL_X, GOAL_Y)

# Número de episodios de entrenamiento
num_episodes = 500
model.agent1.episode_rewards = []  # Lista para almacenar la recompensa por episodio
update_target_every = 20  # Cada cuántos episodios actualizar la red objetivo

for episode in range(num_episodes):
    print(f"Episodio actual: {episode}")
    model.agent1.steps_taken = 0
    model.agent1.x, model.agent1.y = 0, 0  # Volver al punto de inicio
    model.agent1.path = []
    model.agent1.meta_alcanzada = False #Restablecer la bandera
    episode_reward = 0  # Recompensa acumulada en este episodio

    # Ejecutar el episodio hasta que el agente llegue a la meta o se pase del máximo de pasos
    while model.agent1.steps_taken < model.agent1.max_steps:
        prev_x, prev_y = model.agent1.x, model.agent1.y  # Estado previo
        if model.agent1.meta_alcanzada:  # Detener el episodio si la meta fue alcanzada
            break
        model.step()  # Ejecutar un paso en el modelo



        # Calcular la recompensa del episodio
        if (model.agent1.x, model.agent1.y) == (model.agent1.goalx, model.agent1.goaly):
            episode_reward += 10  # Recompensa grande por llegar a la meta
            break
        else:
            episode_reward -= 1  # Penalización por cada paso tomado

    model.agent1.episode_rewards.append(episode_reward)  # Guardar recompensa del episodio


    if episode % 100 == 0:
          print(f"Episodio: {episode}, Recompensa: {episode_reward}, Epsilon: {model.agent1.epsilon}")
          print("AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAaa")

    model.save_log()

model.agent1.plot_results()

def plot_rewards(agent):
        plt.figure(figsize=(10, 5))
        plt.plot(agent.episode_rewards, label="Recompensa por episodio", color="b")
        plt.xlabel("Episodio")
        plt.ylabel("Recompensa acumulada")
        plt.title("Desempeño del agente durante el entrenamiento")
        plt.legend()
        plt.grid()
        plt.show()

# Gráfica de la recompensa promedio por episodio
import matplotlib.pyplot as plt

plt.plot(model.agent1.episode_rewards)
plt.xlabel('Episodios')
plt.ylabel('Recompensa acumulada')
plt.title('Evolución de la recompensa en el entrenamiento')
plt.show()

# Gráfica de la recompensa promedio por episodio
import matplotlib.pyplot as plt

plt.plot(model.agent1.episode_rewards)
plt.xlabel('Episodios')
plt.ylabel('Recompensa acumulada')
plt.title('Evolución de la recompensa en el entrenamiento')
plt.show()

# Una vez finalizado el entrenamiento, graficamos los resultados
plot_rewards(model.agent1)


# Corremos el modelo
for _ in range(80):
    #Aquí andamos tomando los pasos, le pusimos 30 para apreciar que no termina
    #En 25 pasos
    model.step()

# Guardamos los resultados en el log, y el log lo vemos
# <- aca podras encontrar el archivo: walle_log.json
model.save_log()

# Una vez finalizado el entrenamiento, graficamos los resultados
plot_rewards(model.agent1)


# Corremos el modelo
for _ in range(60):
    #Aquí andamos tomando los pasos, le pusimos 30 para apreciar que no termina
    #En 25 pasos
    model.step()

# Guardamos los resultados en el log, y el log lo vemos
# <- aca podras encontrar el archivo: walle_log.json
model.save_log()

# Una vez finalizado el entrenamiento, graficamos los resultados
plot_rewards(model.agent1)


# Corremos el modelo
for _ in range(30):
    #Aquí andamos tomando los pasos, le pusimos 30 para apreciar que no termina
    #En 25 pasos
    model.step()

# Guardamos los resultados en el log, y el log lo vemos
# <- aca podras encontrar el archivo: walle_log.json
model.save_log()

# Evaluar el agente con epsilon = 0 (sin exploración)
model.agent1.epsilon = 0.0
for episode in range(10):
    model.agent1.x, model.agent1.y = 0, 0  # Volver al punto de inicio
    model.agent1.steps_taken = 0
    model.agent1.path = []
    while model.agent1.steps_taken < model.agent1.max_steps:
        model.step()
        if (model.agent1.x, model.agent1.y) == (model.agent1.goalx, model.agent1.goaly):
            print(f"Test Episode {episode+1}: Reached the goal!")
            break